{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive Bayes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "raoRMUU4-iev"
      },
      "source": [
        "#first import the library\r\n",
        "import pandas as pd\r\n",
        "# datasert load\r\n",
        "dataset = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\")\r\n",
        "#url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\r\n",
        "#names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'species']\r\n",
        "# dataset = pd.read_csv(data, names=names)\r\n",
        "print(dataset.head()) #it prints 20 rows of data\r\n",
        "#slicing\r\n",
        "X_features_input = dataset.iloc[:, :-1].values #features[rows, columms]\r\n",
        "print(X_features_input)\r\n",
        "y_label_output = dataset.iloc[:, 4].values #labels\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_features_input, y_label_output, test_size=0.20, random_state=5)\r\n",
        "#x_train = 80% of our features data(input)\r\n",
        "#x_test = 20% of our features data(input)\r\n",
        "#y_train = 80% of our lable data(output)\r\n",
        "#y_test = 20 % of pur lable data(output)\r\n",
        "#imported the algorithms from library\r\n",
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "classifier = GaussianNB()\r\n",
        "# to train the model you have to use the function of \"fit()\"\r\n",
        "# while traininf we only pass the 80 percent of our data\r\n",
        "classifier.fit(X_train, y_train) # X_train = features #y_train= lable\r\n",
        "# now we have to take prediction on testing data\r\n",
        "y_pred = classifier.predict(X_test) #here we only pass the features\r\n",
        "\r\n",
        "#save model \r\n",
        "filename = 'NaiveBayes_model.sav'\r\n",
        "joblib.dump(classifier, 'NaiveBayes_model.sav')\r\n",
        " \r\n",
        "# some time later...\r\n",
        " \r\n",
        "# load the model from disk\r\n",
        "loaded_model = joblib.load('NaiveBayes_model.sav')\r\n",
        "print('Accuracy of loaded model')\r\n",
        "result = loaded_model.score(X_test, y_test)\r\n",
        "print(result)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# from sklearn.metrics import classification_report, confusion_matrix\r\n",
        "# print(confusion_matrix(y_test, y_pred)\r\n",
        "#print(classification_report(y_test, y_pred))\r\n",
        "\r\n",
        "from sklearn.metrics import classification_report, confusion_matrix\r\n",
        "print(confusion_matrix(y_test, y_pred))\r\n",
        "print(classification_report(y_test, y_pred))\r\n",
        "\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "f1_metric = f1_score(y_test, y_pred, average = \"macro\")\r\n",
        "#average=\"macro\" it calculates the sperate precision and recall of\r\n",
        "# each class and than take the average of precision and recall. after it calculate the f1 score\r\n",
        "print(\"F1 Score macro:\",f1_metric)\r\n",
        "#\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "f1_metric_micro = f1_score(y_test, y_pred, average = \"micro\")\r\n",
        "print(\"F1 Score Micro:\",f1_metric_micro)\r\n",
        "# for accuracy\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "print('Accuracy Score: ', accuracy_score(y_pred, y_test)) #y_pred is the output\r\n",
        "# print(tree.plot_tree(classifier))\r\n",
        "from mlxtend.evaluate import bias_variance_decomp\r\n",
        "mse, bias, var = bias_variance_decomp(classifier, X_train, y_train, X_test, y_test, num_rounds=200, random_seed=1)\r\n",
        "# summarize results\r\n",
        "print('Bias: %.3f' % bias)\r\n",
        "print('Variance: %.3f' % var)\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "# clf = svm.SVC(kernel='linear', C=1)\r\n",
        "scores = cross_val_score(classifier.fit(X_train, y_train), X_features_input,y_label_output, cv=5)\r\n",
        "print('Cross Validation')\r\n",
        "print(scores)\r\n",
        "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\r\n",
        "\r\n",
        "#take input from the loaded model\r\n",
        "input_sepal_length = float(input(\"Enter sepal length: \"))\r\n",
        "input_sepal_width = float(input(\"Enter sepal width:\"))\r\n",
        "input_petal_length = float(input(\"Enter petal Length: \"))\r\n",
        "input_petal_width = float(input(\"Enter petal width: \"))\r\n",
        "output = loaded_model.predict([[input_sepal_length, input_sepal_width,input_petal_length,input_petal_width ]])\r\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}